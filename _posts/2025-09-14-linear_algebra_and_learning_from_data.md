---
title: "Linear Algebra and Learning from Data"
date: 2025-09-14
---

# 线性代数与从数据中学习

## 深度学习与神经网络

线性代数、概率/统计和优化是机器学习的数学支柱。这些章节将在神经网络结构之前讲解。但我们认为从目标描述开始更有帮助：**构建一个能正确分类训练数据的函数，从而能够推广到未见过的测试数据** 。

为了使这一表述有意义，你需要更多地了解这个学习函数。这三页的目的正是为接下来的内容提供方向。函数 $$F$$ 的输入是向量、矩阵，有时是张量——每个训练样本对应一个输入 $$v$$ 。以手写数字识别问题为例，每个输入样本是一张图像——一个像素矩阵。我们的目标是将这些图像分别分类为 0 到 9 之间的数字。这十个数字就是学习函数的可能输出。在这个例子中，函数 $$F$$ 学习如何在图像分类中提取有用特征。

MNIST 数据集包含 70,000 个手写数字。我们在其中的一部分上训练学习函数。通过为图像中的不同像素分配权重，我们构建了这个函数。优化这一重大问题（计算的核心）就是选择权重，使得函数能够为输入分配正确的输出 0、1、2、3、4、5、6、7、8 或 9。而我们并不要求完美！（深度学习中的一个危险是**对数据过拟合**。）

接着，我们通过选择未见过的 MNIST 样本并将函数应用于这些测试数据来验证函数的效果。多年来的竞赛推动了测试结果的重大提升。卷积网络现在的错误率已低于 1%。事实上，正是像 MNIST 这样的已知数据集竞赛带来了函数 $$F$$ 结构上的巨大改进。这种结构基于底层神经网络的架构。

### 线性与非线性学习函数

输入是样本 $$v$$ ，输出是计算得到的分类 $$w = F(v)$$ 。最简单的学习函数是线性的： $$w = Av$$ 。矩阵 $$A$$ 中的元素是需要学习的权重：不算太难。通常，函数还会学习一个偏置向量 $$b$$ ，使得 $$F(v) = Av + b$$ 。这个函数是“仿射”的。仿射函数可以被快速学习，但单独使用时过于简单。

更确切地说，线性是一个非常受限的要求。如果 MNIST 使用罗马数字，那么 $$II$$ 可能正好位于 $$I$$ 和 $$III$$ 之间（这是线性所要求的）。但是 I 和 XIX 之间的“中间”又是什么？显然，仿射函数 $$Av + b$$ 并不总是足够的。

非线性可以通过对输入向量 $$v$$ 的各个分量平方来引入。这一步可能有助于将一个圆与其内部的一个点区分开——这是线性函数无法做到的。但 $$F$$ 的构建逐渐转向了具有 $$S$$ 形图像的“Sigmoid 函数”。令人惊讶的是，通过在矩阵 $$A$$ 和 $$B$$ 之间插入这些标准的非线性 $$S$$ 形函数，形成 $$A(S(Bv))$$ ，取得了巨大进展。最终，人们发现这些平滑曲线的逻辑函数 $$S$$ 可以用极其简单的斜坡函数取代，即现在称为 $$ReLU(x)=max(0,x)$$ 。这些非线性“激活函数” $$R$$ 的图像在第 VII.1 节中绘出。

### 神经网络与 F(v) 的结构

产生深度学习的函数形式为 $$F(v) = L(R(L(R(\dots (Lv)))))$$ 。这是仿射函数 $$Lv = Av + b$$ 与作用于向量 $$Lv$$ 每个分量的非线性函数 $$R$$ 的组合。矩阵 **A** 和偏置向量 **b** 是学习函数 **F 中的权重** 。正是这些 **A** 和 **b** 需要通过训练数据来学习，使得输出 $$F(v)$$（几乎）正确。然后 **F** 可以应用于来自同一分布的新样本。如果权重（**A** 和 **b**）选择得当，那么在未见过的测试数据上， $$F(v)$$ 的输出也应该是准确的。在函数 **F** 中增加更多的层通常会提高 $$F(v)$$ 的准确性。

严格来说， $$F(x,v)$$ 取决于输入  **v** 和权重 **x**（所有的 **A** 和 **b**）。第一步输出 $$v_1 = \text{ReLU}(A_1 v + b_1)$$ 构成了我们 **神经网络的第一隐藏层** 。完整的网络从输入层 **v** 开始，到输出层 $$w = F(v)$$ 结束。每一步的仿射部分 $$L_k(v_{k-1}) = A_k v_{k-1} + b_k$$  使用计算得到的权重 $$A_k$$ 和 $$b_k$$ 。

所有这些权重共同组成了深度学习中庞大的优化目标：

**选择权重 $$A_k$$ 和 $$b_k$$ 以最小化所有训练样本的总损失。** 

总损失是每个样本损失的总和。最小二乘的损失函数具有熟悉的形式 $$\|F(v) - \text{true output}\|^2$$ 。不过，最小二乘往往并不是深度学习的最佳损失函数。

<img width="602" height="139" alt="image" src="https://github.com/user-attachments/assets/839f83cf-a4d3-44ba-8a97-8dcae9395b14" />

这里有一张神经网络的图，展示了 $$F(v)$$ 的结构。输入层包含训练样本 $$v = v_0$$ 。输出是它们的分类 $$w = F(v)$$ 。在完美学习的情况下， $$w$$ 将是 0 到 9 之间的（正确）数字。隐藏层为网络增加了深度，正是这种深度使得复合函数 $$F$$ 在深度学习中如此成功。事实上，神经网络中的权重 $$A_{ij}$$ 和 $$b_j$$ 的数量往往大于训练样本 v 的输入数量。

这是一个前馈全连接网络。对于图像，通常使用卷积神经网络（CNN），其权重是共享的——矩阵 $$A$$ 的对角线是恒定的。当架构正确时，深度学习的效果惊人地好。

<img width="909" height="798" alt="image" src="https://github.com/user-attachments/assets/f12502e2-f99c-4168-bc52-37a69eda9a86" />

这个神经网络中的每条对角线都代表一个需要通过优化来学习的权重。方框中的边包含偏置向量 $$b_1$$ 、 $$b_2$$ 、 $$b_3$$ 。其他权重位于 $$A_1$$ 、 $$A_2$$ 、 $$A_3$$  中。

18.065 课程需要一本教材。它在 2017 年的首届课程中开始编写，第一版在 2018 年提供给学生。我非常高兴地承认，这本书的诞生归功于 Ashley C. Fernandes。Ashley 从波士顿收到扫描的页面，并在孟买返回新的章节，准备继续工作。这是我们合作的第七本书，我非常感激。

### The Content of the Book

本书旨在解释数据科学所依赖的数学：线性代数、优化、概率和统计。学习函数中的权重进入矩阵。这些权重通过“随机梯度下降”进行优化。stochastic（=随机的）这个词表明成功受概率而非确定性的支配。大数定律扩展到“大函数定律”：如果架构设计得当、参数计算得当，那么成功的概率就很高。

请注意，这不是一本关于计算、编程或软件的书。许多书在这些方面做得很好。我们最喜欢的一本是 Aunlien Geron 的《Hands-On Machine Learning》（2017，O'Reilly 出版）。此外，来自 TensorFlow、Keras、MathWorks、Caffe 等众多平台的在线帮助，也是对数据科学的重要贡献。

线性代数中有丰富多样的矩阵：对称矩阵、正交矩阵、三角矩阵、带状矩阵、置换矩阵、投影矩阵以及循环矩阵。根据我的经验，正定对称矩阵 S 是王牌。它们具有正的特征值 λ 和正交的特征向量 q。它们是简单秩一投影 qqᵀ 在这些特征向量上的组合：
S = λ₁q₁q₁ᵀ + λ₂q₂q₂ᵀ + ⋯
如果 λ₁ ≥ λ₂ ≥ … 那么 λ₁q₁q₁ᵀ 就是 S 中信息量最丰富的部分。对于样本协方差矩阵，这一部分具有最大的方差。


**第一章**
在我们的一生中，最重要的一步就是将这些思想从对称矩阵扩展到所有矩阵。现在我们需要两组奇异向量 u 和 v。奇异值 σ 替代了特征值 λ。分解
A = σ₁u₁v₁ᵀ + σ₂u₂v₂ᵀ + ⋯
依然成立（这就是 SVD）。随着 σ 的递减，A 的这些秩一部分仍然按重要性排序。关于 A 的“Eckart–Young 定理”补充了我们长期以来对对称矩阵 AᵀA 的认识：对于秩 k，只需保留到 σₖuₖvₖᵀ 即可。

**第二章**
第一章的思想在第二章变成了算法。对于相当大的矩阵，可以计算出 σ、u 和 v。对于非常大的矩阵，我们采用随机化：抽样列和行。对于大量类别的大矩阵，这种方法效果很好。

**第三—第四章**
第三章聚焦于低秩矩阵，第四章聚焦于许多重要的实例。我们在寻找那些使计算特别快（第三章）或特别有用（第四章）的性质。傅里叶矩阵是每一个具有常系数（位置不变）问题的基础。由于 FFT（快速傅里叶变换），这种离散变换速度极快。

**第五章**
第五章尽可能简单地解释我们所需的统计知识。核心思想始终是均值和方差：平均值以及围绕平均值的波动。通常我们可以通过简单的平移将均值降为零。降低方差（不确定性）才是真正的问题。对于随机向量、矩阵和张量，这个问题更为深刻。人们已经认识到，统计学中的线性代数对机器学习至关重要。

**第六章**
第六章介绍两类优化问题。首先是线性规划、二次规划和博弈论中的优雅问题。对偶性和鞍点是关键概念。但深度学习和本书的目标在于别处：规模巨大且结构尽量简单的问题。“导数等于零”依然是基本方程。牛顿法需要的二阶导数数量太多、太复杂，无法计算。即使在每一步下降中用所有数据来降低损失，也常常是不可能的。这就是为什么在每步随机梯度下降中我们只选择一个小批量输入数据。

大规模学习的成功来自一个奇妙的事实：当变量成千上万甚至数百万时，随机化常常能带来可靠性。

**第七章**
第七章从神经网络的架构开始。输入层与隐藏层相连，最终连接到输出层。对于训练数据，输入向量 v 是已知的，正确的输出也已知（通常 w 是 v 的正确分类）。我们优化学习函数 F 中的权重 x，使得 F(x,v) 对几乎所有训练输入 v 都接近 w。

然后将 F 应用于测试数据，这些测试数据来自与训练数据相同的总体。如果 F 学到了所需的内容（且不过拟合：我们不希望用 99 次多项式去拟合 100 个点），那么测试误差也会很低。系统可以识别图像和语音，能够在语言之间翻译。它可能遵循 ImageNet 或 AlexNet 这样的设计，这些都是重大竞赛的获胜者。一个神经网络曾击败围棋世界冠军。

函数 F 通常是分段线性的——权重进入矩阵乘法。隐藏层的每个神经元还有一个非线性的“激活函数”。斜坡函数 ReLU(x) = max(0, x) 现在是压倒性的首选。

在设计组成 F(x,v) 的各层方面，专业知识正在不断增长。我们从全连接层开始——第 n 层的所有神经元都连接到第 n+1 层的所有神经元。卷积神经网络（CNN）通常更好——它们在图像的所有像素上重复相同的权重：这是非常重要的结构。其他层有所不同。池化层降低维度；Dropout 随机丢弃神经元；批归一化（Batch Normalization）重设均值和方差。所有这些步骤构建出一个与训练数据高度匹配的函数。然后 F(x,v) 就可以投入使用了。

# 矩阵字母表

| **A** | 任意矩阵 | **Q** | 正交矩阵 |
|-------|-----------|-------|----------|
| **C** | 循环矩阵 | **R** | 上三角矩阵 |
| **C** | 列矩阵 | **R** | 行矩阵 |
| **D** | 对角矩阵 | **S** | 对称矩阵 |
| **F** | 傅里叶矩阵 | **S** | 样本协方差矩阵 |
| **I** | 单位矩阵 | **T** | 张量 |
| **L** | 下三角矩阵 | **U** | 上三角矩阵 |
| **L** | 拉普拉斯矩阵 | **U** | 左奇异向量 |
| **M** | 混合矩阵 | **V** | 右奇异向量 |
| **M** | 马尔可夫矩阵 | **X** | 特征向量矩阵 |
| **P** | 概率矩阵 | **Λ** | 特征值矩阵 |
| **P** | 投影矩阵 | **Σ** | 奇异值矩阵 |

