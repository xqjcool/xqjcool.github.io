---
title: "Highlights of Linear Algebra"
date: 2025-09-15
---

# 第 I 部分：线性代数要点 

本书的第 I 部分是应用线性代数的严肃介绍。  
如果读者的背景不够扎实或已经遗忘（尤其是数学这一重要部分），**请不要跳过这一部分**。  
这一部分从矩阵列向量的角度开始讲解 $$Ax$$ 和 $$AB$$ 的乘法。这看似形式化，但其实是根本性的。

本章将研究的五个基本问题

| 问题 | 描述 |
|------|------|
| $$Ax=b$$ | 求解线性方程 |
| $$Ax=\lambda x$$ | 特征值问题 |
| $$Av=\sigma u$$ | 奇异值问题 |
| 最小化 $$\|Ax\|^2/\|x\|^2$$ | 最小二乘问题 |
| 分解矩阵 $$A$$ | 矩阵因式分解 |

这些问题看似普通计算题：

- 求 $$x$$  
- 求 $$x$$ 和 $$\lambda$$  
- 求 $$v, u, \sigma$$  
- 将矩阵$$A$$ 表示为“列 $$×$$ 行”

我们关心的是**理解**（甚至超过求解本身）。  
例如我们首先要知道 $$Ax=b$$ 是否有解——“向量 $$b$$ 是否在矩阵 $$A$$ 的列空间里？”  
那个看似简单的词 **“空间”** 意味深长，也非常有用。

特征值方程 $$Ax=\lambda x$$ 与 $$Ax=b$$ 非常不同，因为这里**没有向量 $$b$$**，我们只看矩阵 $$A$$ 本身。  
我们希望找到特征向量方向，使得 $$Ax$$ 与 $$x$$ 保持**同一方向** 。在这个方向上，矩阵的所有复杂连接都消失。  
$$A^2x$$ 就是 $$\lambda^2x$$ 。矩阵 $$e^{At}$$ （来自微分方程）对 $$x$$ 的作用就是乘上 $$e^{\lambda t}$$ 。知道了每个 $$x$$ 和 $$\lambda$$ ，我们就能解任何线性系统。

奇异值方程 $$Av=\sigma u$$ 与特征值问题相似但又不同。 我们有**两个向量** $$v$$ 和 $$u$$ 。矩阵 $$A$$ 可能是矩形的，并且包含大量数据。我们关心数据矩阵中哪些部分重要？ 
**奇异值分解（SVD）** 找到最简单的分块 $$\sigma u v^T$$ 。 这些块是“列向量 $$u$$ ”乘以“行向量 $$v^T$$ ”的形式。每个矩阵都可以由这些正交的块组成。**数据科学与线性代数在 SVD 中相遇。**  
寻找这些块 $$\sigma u v^T$$ 是 主成分分析（PCA）的目标。

最小化与矩阵分解表达了基本的应用问题，它们导向奇异向量 $$v$$ 、 $$u$$ 。例如在最小二乘法中求最优解 $$\hat{x}$$ ，或在 PCA 中找到第一个主成分 $$v_1$$ ,都是让数据最匹配的代数问题。本书不提供代码——我们专注于解释思想。
当你理解了列空间、零空间、特征向量和奇异向量后，你就能准备好各种应用:最小二乘 ,傅里叶变换,统计中的 LASSO, 深度学习中神经网络的随机梯度下降（SGD）。

## 1. 用矩阵的列来理解 $$Ax$$ 的乘法

我们希望你已经熟悉一些线性代数。它是一门美丽的学科——在我们看来比微积分对更多人更有用。但即使是传统的线性代数课程也常常忽略一些基本而重要的事实。本书的这一部分讨论的是**矩阵-向量乘法 \(Ax\)**、矩阵的**列空间**以及**秩**。

我们总是用例子来说明观点。

**例 1** 用 $$A$$ 的三行或两列来乘以 $$x$$ 。

**按行** 

$$
\begin{bmatrix}
2 & 3\\
2 & 4\\
3 & 7
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
=
\begin{bmatrix}
2x_1+3x_2\\
2x_1+4x_2\\
3x_1+7x_2
\end{bmatrix}
$$
